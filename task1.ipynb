{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model, data and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform according to the assignment, parameterized by the expected length\n",
    "class BinaryTransform:\n",
    "    def __init__(self, input_length: int) -> None:\n",
    "        \"\"\"Initializes the single length parameter that defines this transform.\"\"\"\n",
    "        self.input_length = input_length\n",
    "\n",
    "    def __call__(self, binary_data: bytes) -> torch.Tensor:\n",
    "        \"\"\"Returns the (1,input_length) sized Tensor of values between 0 and 1.\n",
    "        The extra dimension is needed for the model's 2D pooling to work properly\n",
    "        (not to be confused with the batch dimension that should be also added,\n",
    "        the final batched model input being of shape (N, 1, L)).\"\"\"\n",
    "        \n",
    "        binary_array = np.frombuffer(binary_data, dtype=np.uint8) \n",
    "        l = len(binary_array)\n",
    "\n",
    "        # Pad or truncate the binary data based on < or > case\n",
    "        # In the == case there is nothing further to do\n",
    "        if l < self.input_length:\n",
    "            padding = np.zeros(self.input_length - l, dtype=np.uint8)\n",
    "            binary_array = np.concatenate((binary_array, padding))\n",
    "        elif l > self.input_length:\n",
    "            # In this case the input should be split into non-overlapping windows...\n",
    "            window_size = ceil(l / self.input_length)\n",
    "            padding = np.zeros(self.input_length * window_size - l, dtype=np.uint8)\n",
    "            binary_array = np.concatenate((binary_array, padding))\n",
    "            # ...and for each window its mean should be taken\n",
    "            binary_array = binary_array.reshape(-1, window_size)\n",
    "            binary_array = np.mean(binary_array, axis=1)\n",
    "            \n",
    "        # Scale the data to [0, 1]\n",
    "        scaled_data = binary_array / 255.0\n",
    "        tensor = torch.tensor(scaled_data, dtype=torch.float32)\n",
    "        # Add extra dimension for the model to work properly.\n",
    "        return tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model according to the assignment on Teams\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(1, 16, kernel_size=(10,), stride=(1,))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.linear = nn.Linear(65496, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pooling(self.relu(self.conv(x)))\n",
    "        x = x.view(-1, 65496) # flatten\n",
    "        x = self.sigmoid(self.linear(x))\n",
    "        return x.reshape(-1) # keep only the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected input length and transform\n",
    "input_length = 2**14\n",
    "transform = BinaryTransform(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Torchvision's DatasetFolder for easier preprocessing fit for the folder structure\n",
    "TRAIN_DATA_PATH = \"data/train\"\n",
    "\n",
    "train_dataset = DatasetFolder(\n",
    "    root=TRAIN_DATA_PATH,\n",
    "    loader=lambda x: open(x, 'rb').read(), # file reading in binary mode\n",
    "    extensions=('',),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Splitting the train set further into train and validation sets by defining index subsets\n",
    "np.random.seed(42)\n",
    "train_indices = np.arange(len(train_dataset))\n",
    "np.random.shuffle(train_indices)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the train and validation subsets\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(train_dataset, val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=64)\n",
    "# Model\n",
    "model = ConvNet().to(device)\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv): Conv1d(1, 16, kernel_size=(10,), stride=(1,))\n",
       "  (relu): ReLU()\n",
       "  (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear): Linear(in_features=65496, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train BCE: 0.00900685 Val. BCE: 0.00395540 Val. Acc. 0.90788224\n",
      "Epoch 1: Train BCE: 0.00300047 Val. BCE: 0.00241600 Val. Acc. 0.95251662\n",
      "Epoch 2: Train BCE: 0.00191229 Val. BCE: 0.00184717 Val. Acc. 0.96296296\n",
      "Epoch 3: Train BCE: 0.00143395 Val. BCE: 0.00153979 Val. Acc. 0.96866097\n",
      "Epoch 4: Train BCE: 0.00118846 Val. BCE: 0.00165777 Val. Acc. 0.96866097\n",
      "Epoch 5: Train BCE: 0.00097784 Val. BCE: 0.00118758 Val. Acc. 0.98005698\n",
      "Epoch 6: Train BCE: 0.00088844 Val. BCE: 0.00082961 Val. Acc. 0.98670465\n",
      "Epoch 7: Train BCE: 0.00067490 Val. BCE: 0.00074352 Val. Acc. 0.99050332\n",
      "Epoch 8: Train BCE: 0.00050040 Val. BCE: 0.00100447 Val. Acc. 0.98290598\n",
      "Epoch 9: Train BCE: 0.00048660 Val. BCE: 0.00061377 Val. Acc. 0.99430199\n"
     ]
    }
   ],
   "source": [
    "# Training loop - even 10 epochs should yield 99% on the validation set without serious overfit\n",
    "torch.manual_seed(42)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    # Define accumulating values\n",
    "    total_train_loss = 0.\n",
    "    total_train_samples = 0\n",
    "    # Forward pass along data\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y.float()) # type conversion needed\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.detach().item()\n",
    "        total_train_samples += len(X)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.\n",
    "    total_val_correct = 0\n",
    "    total_val_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y.float())\n",
    "            pred_label = (y_pred > 0.5)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            total_val_correct += pred_label.eq(y).sum().item()\n",
    "            total_val_samples += len(X)\n",
    "\n",
    "    train_bce = total_train_loss/total_train_samples\n",
    "    val_bce = total_val_loss/total_val_samples\n",
    "    val_acc = total_val_correct / total_val_samples\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train BCE: {train_bce:.8f} Val. BCE: {val_bce:.8f} Val. Acc. {val_acc:.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model for later use\n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryAUROC, BinaryStatScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test set\n",
    "TEST_DATA_PATH = \"data/test\"\n",
    "\n",
    "test_dataset = DatasetFolder(\n",
    "    root=TEST_DATA_PATH,\n",
    "    loader=lambda x: open(x, 'rb').read(),\n",
    "    extensions=('',),\n",
    "    transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv): Conv1d(1, 16, kernel_size=(10,), stride=(1,))\n",
       "  (relu): ReLU()\n",
       "  (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear): Linear(in_features=65496, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading model from file: no need to run the training loop adain if the runtime is restarted\n",
    "model = ConvNet().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop with additional metrics\n",
    "roc_metric = BinaryAUROC()\n",
    "stat_scores = BinaryStatScores()\n",
    "\n",
    "total_test_correct = 0\n",
    "total_test_samples = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        pred_label = (y_pred > 0.5)\n",
    "\n",
    "        total_test_correct += pred_label.eq(y).sum().item()\n",
    "        total_test_samples += len(X)\n",
    "\n",
    "        roc_metric.update(pred_label.float(), y.float())\n",
    "        stat_scores.update(pred_label.float(), y.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_metric.compute()\n",
    "tp, fp, tn, fn, _ = stat_scores.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Acc. on test set: 0.99487179\n",
      "ROC AUC: 0.99487185\n",
      "TPR: 0.99487180\n",
      "TNR: 0.99487180\n",
      "FPR: 0.00512821\n",
      "FNR: 0.00512821\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Acc. on test set: {total_test_correct / total_test_samples:.8f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.8f}\")\n",
    "print(f\"TPR: {(tp / (tp + fn)).item():.8f}\")\n",
    "print(f\"TNR: {(tn / (tn + fp)).item():.8f}\")\n",
    "print(f\"FPR: {(fp / (fp + tn)).item():.8f}\")\n",
    "print(f\"FNR: {(fn / (fn + tp)).item():.8f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
