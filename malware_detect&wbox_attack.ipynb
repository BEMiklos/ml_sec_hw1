{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.085474004Z",
     "start_time": "2024-04-05T11:08:14.509605562Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "from torchmetrics.classification import AUROC, StatScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "441cc6079ab25645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.126359585Z",
     "start_time": "2024-04-05T11:08:17.120486735Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BinaryTransform:\n",
    "    def __init__(self, input_length):\n",
    "        self.input_length = input_length\n",
    "\n",
    "    def __call__(self, binary_data):\n",
    "        binary_data = np.frombuffer(binary_data, dtype=np.uint8)\n",
    "        \n",
    "        l = len(binary_data)\n",
    "\n",
    "        # Pad or truncate the binary data\n",
    "        if l < self.input_length:\n",
    "            padding = np.zeros(self.input_length - l, dtype=np.uint8)\n",
    "            binary_data = np.concatenate((binary_data, padding))\n",
    "        elif l > self.input_length:\n",
    "            excess = ceil(l / self.input_length)\n",
    "            padding = np.zeros(self.input_length * excess - l, dtype=np.uint8)\n",
    "            binary_data = np.concatenate((binary_data, padding))\n",
    "            binary_data = binary_data.reshape(len(binary_data)//excess, -1)\n",
    "            binary_data = np.mean(binary_data, axis=1)\n",
    "            \n",
    "        # Scale the data to [0, 1]\n",
    "        scaled_data = binary_data / 255.0\n",
    "        tensor = torch.tensor(scaled_data, dtype=torch.float32)\n",
    "        return tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ca22ddcd6fa7a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.128088446Z",
     "start_time": "2024-04-05T11:08:17.120761619Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There are two versions of the assigment, so we created two versions, either of them works\n",
    "\n",
    "# Assignment on Teams\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=(10,), stride=(1,))\n",
    "        self.fc1 = nn.Linear(65496, 1)  # Adjust the input size based on your data size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        x = x.view(-1, 65496)\n",
    "        return F.sigmoid(self.fc1(x))\n",
    "\n",
    "# Assignmanet on Moodle (Linear input size adjusted to meet expected output dimension)\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(1, 16, kernel_size=(10,), stride=(1,))\n",
    "#         self.fc1 = nn.Linear(2*65488, 1)  # Adjust the input size based on your data size\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.max_pool1d(x, kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "#         x = x.view(-1, 2*65488)\n",
    "#         return F.sigmoid(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374cf0282921e26d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.128983520Z",
     "start_time": "2024-04-05T11:08:17.120848049Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc5a0710030a7284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.153053760Z",
     "start_time": "2024-04-05T11:08:17.120933920Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "train_data_path = \"data/train\"\n",
    "test_data_path = \"data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8756b81cdb12b712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.167897717Z",
     "start_time": "2024-04-05T11:08:17.121019397Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the input length and instantiate the transform\n",
    "input_length = 16384\n",
    "transform = BinaryTransform(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42364c72a6eac880",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.169078052Z",
     "start_time": "2024-04-05T11:08:17.165462696Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "train_dataset = DatasetFolder(root=train_data_path, loader=lambda x: open(x, 'rb').read(), extensions=('',), transform=transform)\n",
    "test_dataset = DatasetFolder(root=test_data_path, loader=lambda x: open(x, 'rb').read(), extensions=('',), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d80afdb333ecaf63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.169902355Z",
     "start_time": "2024-04-05T11:08:17.165595167Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split dataset into train and validation sets\n",
    "indices = np.arange(len(train_dataset))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dff6cbca3c93545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.171084929Z",
     "start_time": "2024-04-05T11:08:17.165759395Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(torch.utils.data.Subset(train_dataset, train_indices), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(torch.utils.data.Subset(train_dataset, val_indices), batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dc4d5c56805bbb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.171956265Z",
     "start_time": "2024-04-05T11:08:17.165839809Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3d2c2c709645436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.185407071Z",
     "start_time": "2024-04-05T11:08:17.170300907Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1            [-1, 16, 16375]             176\n",
      "            Linear-2                    [-1, 1]          65,497\n",
      "================================================================\n",
      "Total params: 65,673\n",
      "Trainable params: 65,673\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 2.00\n",
      "Params size (MB): 0.25\n",
      "Estimated Total Size (MB): 2.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(1,16384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32590b445a9949b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.203527617Z",
     "start_time": "2024-04-05T11:08:17.183917430Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31f3db7460488d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.204120799Z",
     "start_time": "2024-04-05T11:08:17.187983409Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up TensorBoard\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb3ece0c1552eb8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.237611197Z",
     "start_time": "2024-04-05T11:08:17.193408719Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "patience = 4\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e4f2367b67933e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:11:31.229866251Z",
     "start_time": "2024-04-05T11:09:34.057256520Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.015413649771385285, Val Loss: 0.0047024543780094085, Val Acc: 87.55935422602089\n",
      "Epoch 1, Train Loss: 0.0030089689037350554, Val Loss: 0.002972913504439315, Val Acc: 94.01709401709402\n",
      "Epoch 2, Train Loss: 0.0020407564971424673, Val Loss: 0.0025795027825907084, Val Acc: 94.3019943019943\n",
      "Epoch 3, Train Loss: 0.0016685363191824693, Val Loss: 0.002696676627296781, Val Acc: 93.44729344729345\n",
      "Epoch 4, Train Loss: 0.0012676939959563906, Val Loss: 0.001959722160104333, Val Acc: 95.53656220322887\n",
      "Epoch 5, Train Loss: 0.0009891373754657522, Val Loss: 0.0012483850956415971, Val Acc: 97.72079772079772\n",
      "Epoch 6, Train Loss: 0.0008004888809541158, Val Loss: 0.0017632039163017544, Val Acc: 95.82146248812916\n",
      "Epoch 7, Train Loss: 0.0007825787453982322, Val Loss: 0.0011610416067518287, Val Acc: 97.62583095916429\n",
      "Epoch 8, Train Loss: 0.0005888685013221006, Val Loss: 0.0010606287434654698, Val Acc: 97.81576448243115\n",
      "Epoch 9, Train Loss: 0.0006392580124606046, Val Loss: 0.001321809283207398, Val Acc: 96.77113010446344\n",
      "Epoch 10, Train Loss: 0.0005597578214559557, Val Loss: 0.0008926721880564436, Val Acc: 98.2905982905983\n",
      "Epoch 11, Train Loss: 0.0005226489876677071, Val Loss: 0.0008093202336278283, Val Acc: 98.48053181386514\n",
      "Epoch 12, Train Loss: 0.0005007719351558677, Val Loss: 0.0015281142130961105, Val Acc: 96.67616334283001\n",
      "Epoch 13, Train Loss: 0.0006001095447081301, Val Loss: 0.0007150524206537354, Val Acc: 98.38556505223171\n",
      "Epoch 14, Train Loss: 0.0002567767501817421, Val Loss: 0.0007791644305429454, Val Acc: 98.38556505223171\n",
      "Epoch 15, Train Loss: 0.00019609761084356006, Val Loss: 0.000640193534199527, Val Acc: 98.76543209876543\n",
      "Epoch 16, Train Loss: 0.00018381123405630676, Val Loss: 0.0007093430028879857, Val Acc: 98.76543209876543\n",
      "Epoch 17, Train Loss: 0.0001686581370288466, Val Loss: 0.000614326197834436, Val Acc: 99.05033238366572\n",
      "Epoch 18, Train Loss: 0.00014216630989863808, Val Loss: 0.0006092868140365323, Val Acc: 98.86039886039886\n",
      "Epoch 19, Train Loss: 0.00013872693259819378, Val Loss: 0.0005997465764228095, Val Acc: 99.05033238366572\n",
      "Epoch 20, Train Loss: 0.00014146403752579698, Val Loss: 0.000562987510550056, Val Acc: 98.95536562203229\n",
      "Epoch 21, Train Loss: 0.00011792324157791818, Val Loss: 0.0005902450638739188, Val Acc: 99.05033238366572\n",
      "Epoch 22, Train Loss: 0.00010347031955226292, Val Loss: 0.0006294197912335906, Val Acc: 98.76543209876543\n",
      "Epoch 23, Train Loss: 9.690784666525017e-05, Val Loss: 0.0005722363140692285, Val Acc: 98.95536562203229\n",
      "Epoch 24, Train Loss: 8.973186181029581e-05, Val Loss: 0.0006130088221525874, Val Acc: 98.76543209876543\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # You can adjust the number of epochs\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).squeeze()\n",
    "            loss = criterion(output, target.float())\n",
    "            val_loss += loss.item()\n",
    "            total += target.size(0)\n",
    "            pred_label = output > 0.5\n",
    "            correct += pred_label.eq(target).sum().item()\n",
    "        \n",
    "    # Write to TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss/len(train_loader.dataset), epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss/len(val_loader.dataset), epoch)\n",
    "    writer.add_scalar('Accuracy/val', 100.*correct/total, epoch)\n",
    "\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss/len(train_loader.dataset)}, Val Loss: {val_loss/len(val_loader.dataset)}, Val Acc: {100.*correct/total}')\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ee9078c6617fa90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:18.011098572Z",
     "start_time": "2024-04-05T11:08:18.007769501Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Close TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0711652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f436560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = ConvNet().to(device)\n",
    "saved_model.load_state_dict(torch.load(\"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92d110bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_metric = AUROC(task=\"binary\")\n",
    "stat_scores = StatScores(task=\"binary\")\n",
    "\n",
    "test_loss = 0\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = saved_model(data).squeeze()\n",
    "        loss = criterion(output, target.float())\n",
    "        test_loss += loss.item()\n",
    "        total += target.size(0)\n",
    "        pred_label = output > 0.5\n",
    "        correct += pred_label.eq(target).sum().item()\n",
    "        roc_metric.update(pred_label.float(), target.float())\n",
    "        stat_scores.update(pred_label.float(), target.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fefabaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mean acc. on test set: ', 0.9923076923076923)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Mean acc. on test set: \", correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "628e7a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ROC AUC: ', 0.992307722568512)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ROC AUC: \", roc_metric.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59c8ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp, fp, tn, fn = stat_scores.compute()\n",
    "scores = stat_scores.compute()\n",
    "tp = scores[0]\n",
    "fp = scores[1]\n",
    "tn = scores[2]\n",
    "fn = scores[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95e09dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TPR: ', 0.9948717951774597)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"TPR: \", (tp / (tp + fn)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6023587e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TNR: ', 0.9897435903549194)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"TNR: \", (tn / (tn + fp)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e4e4feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('FPR: ', 0.010256410576403141)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"FPR: \", (fp / (fp + tn)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da5e18a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('FNR: ', 0.0051282052882015705)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"FNR: \", (fn / (fn + tp)).item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
