{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:41:42.402639650Z",
     "start_time": "2024-04-04T21:41:40.696222627Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BinaryTransform:\n",
    "    def __init__(self, input_length):\n",
    "        self.input_length = input_length\n",
    "\n",
    "    def __call__(self, binary_data):\n",
    "        binary_data = np.frombuffer(binary_data, dtype=np.uint8)\n",
    "        \n",
    "        l = len(binary_data)\n",
    "\n",
    "        # Pad or truncate the binary data\n",
    "        if l < self.input_length:\n",
    "            padding = np.zeros(self.input_length - l, dtype=np.uint8)\n",
    "            binary_data = np.concatenate((binary_data, padding))\n",
    "        elif l > self.input_length:\n",
    "            excess = ceil(l / self.input_length)\n",
    "            padding = np.zeros(self.input_length * excess - l, dtype=np.uint8)\n",
    "            binary_data = np.concatenate((binary_data, padding))\n",
    "            binary_data = binary_data.reshape(len(binary_data)//excess, -1)\n",
    "            binary_data = np.mean(binary_data, axis=1)\n",
    "            \n",
    "        # Scale the data to [0, 1]\n",
    "        scaled_data = binary_data / 255.0\n",
    "\n",
    "        return torch.tensor(scaled_data, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:41:43.147833675Z",
     "start_time": "2024-04-04T21:41:43.135795133Z"
    }
   },
   "id": "441cc6079ab25645",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(files) for _, _, files in os.walk(self.root_dir)])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        class_name = self.classes[idx // 2]\n",
    "        class_dir = os.path.join(self.root_dir, class_name)\n",
    "        files = os.listdir(class_dir)\n",
    "        file_name = files[idx % len(files)]\n",
    "        file_path = os.path.join(class_dir, file_name)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            binary_data = f.read()\n",
    "        label = 1 if class_name == 'malware' else 0\n",
    "        if self.transform:\n",
    "            binary_data = self.transform(binary_data)\n",
    "        return binary_data, label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:41:46.092522017Z",
     "start_time": "2024-04-04T21:41:46.077255144Z"
    }
   },
   "id": "96ebf5b3fd53dd9a",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the convolutional network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=(10,), stride=(1,))\n",
    "        self.fc1 = nn.Linear(65488, 1)  # Adjust the input size based on your data size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        return F.sigmoid(self.fc1(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:41:47.883788172Z",
     "start_time": "2024-04-04T21:41:47.870740991Z"
    }
   },
   "id": "e8ca22ddcd6fa7a7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:41:48.807233183Z",
     "start_time": "2024-04-04T21:41:48.791208204Z"
    }
   },
   "id": "374cf0282921e26d",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "train_data_path = \"./data/train\"\n",
    "test_data_path = \"./data/test\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:41:50.116506651Z",
     "start_time": "2024-04-04T21:41:50.107864755Z"
    }
   },
   "id": "fc5a0710030a7284",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the input length and instantiate the transform\n",
    "input_length = 16384\n",
    "transform = BinaryTransform(input_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:41:51.988710886Z",
     "start_time": "2024-04-04T21:41:51.980158148Z"
    }
   },
   "id": "8756b81cdb12b712",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = BinaryDataset(train_data_path, transform= transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:42:03.058879905Z",
     "start_time": "2024-04-04T21:42:03.042820352Z"
    }
   },
   "id": "42364c72a6eac880",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Split dataset into train and validation sets\n",
    "train_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:42:05.749484504Z",
     "start_time": "2024-04-04T21:42:05.741638571Z"
    }
   },
   "id": "d80afdb333ecaf63",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create train and validation datasets and dataloaders\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:42:07.069184840Z",
     "start_time": "2024-04-04T21:42:07.058227147Z"
    }
   },
   "id": "f4c4cdb9f4eef0e6",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:42:09.337945794Z",
     "start_time": "2024-04-04T21:42:09.332164861Z"
    }
   },
   "id": "5dff6cbca3c93545",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = ConvNet().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:42:09.898380324Z",
     "start_time": "2024-04-04T21:42:09.890348212Z"
    }
   },
   "id": "5dc4d5c56805bbb6",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1            [-1, 16, 16375]             176\n",
      "            Linear-2                    [-1, 1]          65,489\n",
      "================================================================\n",
      "Total params: 65,665\n",
      "Trainable params: 65,665\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 2.00\n",
      "Params size (MB): 0.25\n",
      "Estimated Total Size (MB): 2.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(1,16384))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:42:11.178211488Z",
     "start_time": "2024-04-04T21:42:11.134394591Z"
    }
   },
   "id": "c3d2c2c709645436",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:42:16.726427135Z",
     "start_time": "2024-04-04T21:42:15.985759026Z"
    }
   },
   "id": "32590b445a9949b0",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set up TensorBoard\n",
    "writer = SummaryWriter()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:42:17.592236064Z",
     "start_time": "2024-04-04T21:42:17.579633019Z"
    }
   },
   "id": "31f3db7460488d37",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:42:19.964580853Z",
     "start_time": "2024-04-04T21:42:19.959563507Z"
    }
   },
   "id": "cb3ece0c1552eb8b",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m      3\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (data, target) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[1;32m      5\u001B[0m     data, target \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device), target\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      6\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/anaconda3/envs/aisec/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/aisec/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/anaconda3/envs/aisec/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/aisec/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[0;34m(self, indices)\u001B[0m\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[0;32m~/anaconda3/envs/aisec/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "Cell \u001B[0;32mIn[3], line 12\u001B[0m, in \u001B[0;36mBinaryDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[0;32m---> 12\u001B[0m     class_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses[idx \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m     13\u001B[0m     class_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_dir, class_name)\n\u001B[1;32m     14\u001B[0m     files \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mlistdir(class_dir)\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # You can adjust the number of epochs\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "    # Write to TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss/len(train_loader.dataset), epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss/len(val_loader.dataset), epoch)\n",
    "    writer.add_scalar('Accuracy/val', 100.*correct/total, epoch)\n",
    "\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss/len(train_loader.dataset)}, Val Loss: {val_loss/len(val_loader.dataset)}, Val Acc: {100.*correct/total}')\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:44:48.033629873Z",
     "start_time": "2024-04-04T21:44:47.967214406Z"
    }
   },
   "id": "d2e4f2367b67933e",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Close TensorBoard writer\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ee9078c6617fa90"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
