{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:14.640211072Z",
     "start_time": "2024-04-06T17:08:12.411184843Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bemiklos/anaconda3/envs/aisec/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/bemiklos/anaconda3/envs/aisec/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "from torchmetrics.classification import AUROC, StatScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "441cc6079ab25645",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:15.521848324Z",
     "start_time": "2024-04-06T17:08:15.503967494Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryTransform:\n",
    "    def __init__(self, input_length):\n",
    "        self.input_length = input_length\n",
    "\n",
    "    def __call__(self, binary_data):\n",
    "        binary_data = np.frombuffer(binary_data, dtype=np.uint8)\n",
    "        \n",
    "        l = len(binary_data)\n",
    "\n",
    "        # Pad or truncate the binary data\n",
    "        if l < self.input_length:\n",
    "            padding = np.zeros(self.input_length - l, dtype=np.uint8)\n",
    "            binary_data = np.concatenate((binary_data, padding))\n",
    "        elif l > self.input_length:\n",
    "            excess = ceil(l / self.input_length)\n",
    "            padding = np.zeros(self.input_length * excess - l, dtype=np.uint8)\n",
    "            binary_data = np.concatenate((binary_data, padding))\n",
    "            binary_data = binary_data.reshape(len(binary_data)//excess, -1)\n",
    "            binary_data = np.mean(binary_data, axis=1)\n",
    "            \n",
    "        # Scale the data to [0, 1]\n",
    "        scaled_data = binary_data / 255.0\n",
    "        tensor = torch.tensor(scaled_data, dtype=torch.float32)\n",
    "        return tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ca22ddcd6fa7a7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:16.825406198Z",
     "start_time": "2024-04-06T17:08:16.815071116Z"
    }
   },
   "outputs": [],
   "source": [
    "# There are two versions of the assigment, so we created two versions, either of them works\n",
    "\n",
    "# Assignment on Teams\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=(10,), stride=(1,))\n",
    "        self.fc1 = nn.Linear(65496, 1)  # Adjust the input size based on your data size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        x = x.view(-1, 65496)\n",
    "        return F.sigmoid(self.fc1(x))\n",
    "\n",
    "# Assignmanet on Moodle (Linear input size adjusted to meet expected output dimension)\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(1, 16, kernel_size=(10,), stride=(1,))\n",
    "#         self.fc1 = nn.Linear(2*65488, 1)  # Adjust the input size based on your data size\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.max_pool1d(x, kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "#         x = x.view(-1, 2*65488)\n",
    "#         return F.sigmoid(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374cf0282921e26d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:17.510351249Z",
     "start_time": "2024-04-06T17:08:17.503314775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc5a0710030a7284",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:18.050807301Z",
     "start_time": "2024-04-06T17:08:18.041445190Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "train_data_path = \"data/train\"\n",
    "test_data_path = \"data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8756b81cdb12b712",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:18.514317915Z",
     "start_time": "2024-04-06T17:08:18.507416633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the input length and instantiate the transform\n",
    "input_length = 16384\n",
    "transform = BinaryTransform(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42364c72a6eac880",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:19.021753032Z",
     "start_time": "2024-04-06T17:08:19.004206072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "train_dataset = DatasetFolder(root=train_data_path, loader=lambda x: open(x, 'rb').read(), extensions=('',), transform=transform)\n",
    "test_dataset = DatasetFolder(root=test_data_path, loader=lambda x: open(x, 'rb').read(), extensions=('',), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80afdb333ecaf63",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:19.481042460Z",
     "start_time": "2024-04-06T17:08:19.456230200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into train and validation sets\n",
    "indices = np.arange(len(train_dataset))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dff6cbca3c93545",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:19.953649546Z",
     "start_time": "2024-04-06T17:08:19.947314703Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(torch.utils.data.Subset(train_dataset, train_indices), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(torch.utils.data.Subset(train_dataset, val_indices), batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dc4d5c56805bbb6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:20.623176482Z",
     "start_time": "2024-04-06T17:08:20.614868469Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d2c2c709645436",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:21.362788517Z",
     "start_time": "2024-04-06T17:08:21.338786596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1            [-1, 16, 16375]             176\n",
      "            Linear-2                    [-1, 1]          65,497\n",
      "================================================================\n",
      "Total params: 65,673\n",
      "Trainable params: 65,673\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 2.00\n",
      "Params size (MB): 0.25\n",
      "Estimated Total Size (MB): 2.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(1,16384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32590b445a9949b0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:22.697854351Z",
     "start_time": "2024-04-06T17:08:22.018516294Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb3ece0c1552eb8b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:08:22.701951111Z",
     "start_time": "2024-04-06T17:08:22.700280502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "patience = 4\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2e4f2367b67933e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:10:03.988924971Z",
     "start_time": "2024-04-06T17:08:23.422922332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.012068889158791918, Val Loss: 0.0035988709271123948, Val Acc: 93.82716049382717\n",
      "Epoch 1, Train Loss: 0.003090372425077599, Val Loss: 0.0025605520008862414, Val Acc: 93.73219373219374\n",
      "Epoch 2, Train Loss: 0.002068530737832158, Val Loss: 0.0018050057666707017, Val Acc: 96.77113010446344\n",
      "Epoch 3, Train Loss: 0.0015388828515399992, Val Loss: 0.0015230983255142727, Val Acc: 96.9610636277303\n",
      "Epoch 4, Train Loss: 0.0013180894213912445, Val Loss: 0.001265048386364581, Val Acc: 97.62583095916429\n",
      "Epoch 5, Train Loss: 0.0009429900655670772, Val Loss: 0.001143952826319257, Val Acc: 97.91073124406458\n",
      "Epoch 6, Train Loss: 0.0007922101400468133, Val Loss: 0.001160141950913644, Val Acc: 98.38556505223171\n",
      "Epoch 7, Train Loss: 0.0006616503890265669, Val Loss: 0.0009018354124945906, Val Acc: 98.76543209876543\n",
      "Epoch 8, Train Loss: 0.0005480432147550592, Val Loss: 0.0007136163194747846, Val Acc: 98.86039886039886\n",
      "Epoch 9, Train Loss: 0.00042826019297249684, Val Loss: 0.000673281812486712, Val Acc: 98.670465337132\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # You can adjust the number of epochs\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).squeeze()\n",
    "            loss = criterion(output, target.float())\n",
    "            val_loss += loss.item()\n",
    "            total += target.size(0)\n",
    "            pred_label = output > 0.5\n",
    "            correct += pred_label.eq(target).sum().item()\n",
    "\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss/len(train_loader.dataset)}, Val Loss: {val_loss/len(val_loader.dataset)}, Val Acc: {100.*correct/total}')\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0711652c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:10:41.156060395Z",
     "start_time": "2024-04-06T17:10:41.107036839Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f436560",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:10:42.257023869Z",
     "start_time": "2024-04-06T17:10:42.195627191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = ConvNet().to(device)\n",
    "saved_model.load_state_dict(torch.load(\"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92d110bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:10:45.196080956Z",
     "start_time": "2024-04-06T17:10:43.894407966Z"
    }
   },
   "outputs": [],
   "source": [
    "roc_metric = AUROC(task=\"binary\")\n",
    "stat_scores = StatScores(task=\"binary\")\n",
    "\n",
    "test_loss = 0\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = saved_model(data).squeeze()\n",
    "        loss = criterion(output, target.float())\n",
    "        test_loss += loss.item()\n",
    "        total += target.size(0)\n",
    "        pred_label = output > 0.5\n",
    "        correct += pred_label.eq(target).sum().item()\n",
    "        roc_metric.update(pred_label.float(), target.float())\n",
    "        stat_scores.update(pred_label.float(), target.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fefabaeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:10:45.196235748Z",
     "start_time": "2024-04-06T17:10:45.193958676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('Mean acc. on test set: ', 0.9897435897435898)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Mean acc. on test set: \", correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "628e7a87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:10:45.323020148Z",
     "start_time": "2024-04-06T17:10:45.279288967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('ROC AUC: ', 0.9897435903549194)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ROC AUC: \", roc_metric.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59c8ff1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:10:46.293118283Z",
     "start_time": "2024-04-06T17:10:46.287913948Z"
    }
   },
   "outputs": [],
   "source": [
    "# tp, fp, tn, fn = stat_scores.compute()\n",
    "scores = stat_scores.compute()\n",
    "tp = scores[0]\n",
    "fp = scores[1]\n",
    "tn = scores[2]\n",
    "fn = scores[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95e09dde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:10:47.111655426Z",
     "start_time": "2024-04-06T17:10:47.102188785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('TPR: ', 0.9897435903549194)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"TPR: \", (tp / (tp + fn)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6023587e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:10:47.626337283Z",
     "start_time": "2024-04-06T17:10:47.618628219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('TNR: ', 0.9897435903549194)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"TNR: \", (tn / (tn + fp)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e4e4feb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:10:48.088616876Z",
     "start_time": "2024-04-06T17:10:48.081413300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('FPR: ', 0.010256410576403141)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"FPR: \", (fp / (fp + tn)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da5e18a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:10:48.628480666Z",
     "start_time": "2024-04-06T17:10:48.603939230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('FNR: ', 0.010256410576403141)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"FNR: \", (fn / (fn + tp)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# White-box PGD attack"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13fb70ad3c446d2c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eps = 8/255\n",
    "alpha = 2/255\n",
    "steps = 10\n",
    "batch_size = 64"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:10:58.339889294Z",
     "start_time": "2024-04-06T17:10:58.323554087Z"
    }
   },
   "id": "7b330dc2a639f4c9",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "victim_path = \"data/victim\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:11:00.479143767Z",
     "start_time": "2024-04-06T17:11:00.469030735Z"
    }
   },
   "id": "aa64c51e7e67186b",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "victim_dataset = DatasetFolder(root=train_data_path, loader=lambda x: open(x, 'rb').read(), extensions=('',), transform=transform)\n",
    "victim_loader = DataLoader(victim_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:11:02.126975337Z",
     "start_time": "2024-04-06T17:11:02.100620300Z"
    }
   },
   "id": "956c3afa7407083b",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pgd_attack(inputs,labels):\n",
    "    adv_inputs = inputs.clone().detach()\n",
    "    for _ in range(steps):\n",
    "        adv_inputs.requires_grad = True\n",
    "        outputs = model(adv_inputs)\n",
    "        \n",
    "        target_labels = torch.zeros(labels.size()).unsqueeze(1)\n",
    "        \n",
    "        cost = -criterion(outputs,target_labels)\n",
    "        \n",
    "        grad = torch.autograd.grad(\n",
    "            cost, adv_inputs, retain_graph=False, create_graph=False\n",
    "        )[0]\n",
    "        \n",
    "        adv_inputs = adv_inputs.detach() + alpha * grad.sign()\n",
    "        delta = torch.clamp(adv_inputs - inputs, min=-eps, max=eps)\n",
    "        adv_inputs = torch.clamp(inputs + delta, min=0, max=1).detach()\n",
    "        \n",
    "    return adv_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:11:02.734047332Z",
     "start_time": "2024-04-06T17:11:02.727402489Z"
    }
   },
   "id": "2f110be827bc6e8f",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pgd_correct = 0\n",
    "model.eval()\n",
    "for inputs, labels in victim_loader:    \n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    adv_inputs = pgd_attack(inputs,labels)\n",
    "    outputs = model(adv_inputs)\n",
    "    _,predicted = torch.max(outputs.data,1)\n",
    "    pgd_correct += (predicted == labels).sum().item()\n",
    "pgd_accuracy = pgd_correct / len(test_loader.dataset)\n",
    "print(f\"Attack accuracy: {pgd_accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e523ed6b5e110028",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9cc7384a6b43af6a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
