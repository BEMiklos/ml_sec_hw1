{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.085474004Z",
     "start_time": "2024-04-05T11:08:14.509605562Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "441cc6079ab25645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.126359585Z",
     "start_time": "2024-04-05T11:08:17.120486735Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BinaryTransform:\n",
    "    def __init__(self, input_length):\n",
    "        self.input_length = input_length\n",
    "\n",
    "    def __call__(self, binary_data):\n",
    "        binary_data = np.frombuffer(binary_data, dtype=np.uint8)\n",
    "        \n",
    "        l = len(binary_data)\n",
    "\n",
    "        # Pad or truncate the binary data\n",
    "        if l < self.input_length:\n",
    "            padding = np.zeros(self.input_length - l, dtype=np.uint8)\n",
    "            binary_data = np.concatenate((binary_data, padding))\n",
    "        elif l > self.input_length:\n",
    "            excess = ceil(l / self.input_length)\n",
    "            padding = np.zeros(self.input_length * excess - l, dtype=np.uint8)\n",
    "            binary_data = np.concatenate((binary_data, padding))\n",
    "            binary_data = binary_data.reshape(len(binary_data)//excess, -1)\n",
    "            binary_data = np.mean(binary_data, axis=1)\n",
    "            \n",
    "        # Scale the data to [0, 1]\n",
    "        scaled_data = binary_data / 255.0\n",
    "        tensor = torch.tensor(scaled_data, dtype=torch.float32)\n",
    "        return tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8ca22ddcd6fa7a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.128088446Z",
     "start_time": "2024-04-05T11:08:17.120761619Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There are two versions of the assigment, so we created two versions, either of them works\n",
    "\n",
    "# Assignment on Teams\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=(10,), stride=(1,))\n",
    "        self.fc1 = nn.Linear(65496, 1)  # Adjust the input size based on your data size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        x = x.view(-1, 65496)\n",
    "        return F.sigmoid(self.fc1(x))\n",
    "\n",
    "# Assignmanet on Moodle (Linear input size adjusted to meet expected output dimension)\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(1, 16, kernel_size=(10,), stride=(1,))\n",
    "#         self.fc1 = nn.Linear(2*65488, 1)  # Adjust the input size based on your data size\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.max_pool1d(x, kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "#         x = x.view(-1, 2*65488)\n",
    "#         return F.sigmoid(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "374cf0282921e26d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.128983520Z",
     "start_time": "2024-04-05T11:08:17.120848049Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc5a0710030a7284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.153053760Z",
     "start_time": "2024-04-05T11:08:17.120933920Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "train_data_path = \"data/train\"\n",
    "test_data_path = \"data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8756b81cdb12b712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.167897717Z",
     "start_time": "2024-04-05T11:08:17.121019397Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the input length and instantiate the transform\n",
    "input_length = 16384\n",
    "transform = BinaryTransform(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42364c72a6eac880",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.169078052Z",
     "start_time": "2024-04-05T11:08:17.165462696Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = DatasetFolder(root=train_data_path, loader=lambda x: open(x, 'rb').read(), extensions=('',), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d80afdb333ecaf63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.169902355Z",
     "start_time": "2024-04-05T11:08:17.165595167Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split dataset into train and validation sets\n",
    "train_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4c4cdb9f4eef0e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.170433696Z",
     "start_time": "2024-04-05T11:08:17.165674843Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create train and validation datasets and dataloaders\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dff6cbca3c93545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.171084929Z",
     "start_time": "2024-04-05T11:08:17.165759395Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dc4d5c56805bbb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.171956265Z",
     "start_time": "2024-04-05T11:08:17.165839809Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3d2c2c709645436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.185407071Z",
     "start_time": "2024-04-05T11:08:17.170300907Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1            [-1, 16, 16375]             176\n",
      "            Linear-2                    [-1, 1]          65,497\n",
      "================================================================\n",
      "Total params: 65,673\n",
      "Trainable params: 65,673\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 2.00\n",
      "Params size (MB): 0.25\n",
      "Estimated Total Size (MB): 2.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(1,16384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32590b445a9949b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.203527617Z",
     "start_time": "2024-04-05T11:08:17.183917430Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31f3db7460488d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.204120799Z",
     "start_time": "2024-04-05T11:08:17.187983409Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up TensorBoard\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb3ece0c1552eb8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:17.237611197Z",
     "start_time": "2024-04-05T11:08:17.193408719Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2e4f2367b67933e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:11:31.229866251Z",
     "start_time": "2024-04-05T11:09:34.057256520Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.010870568454265594, Val Loss: 0.010861280721816581, Val Acc: 15.527065527065528\n",
      "Epoch 1, Train Loss: 0.01086128063690968, Val Loss: 0.010861280721816581, Val Acc: 19.08831908831909\n",
      "Epoch 2, Train Loss: 0.01086128063690968, Val Loss: 0.010861280721816581, Val Acc: 19.08831908831909\n",
      "Epoch 3, Train Loss: 0.01086128063690968, Val Loss: 0.010861280721816581, Val Acc: 19.08831908831909\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # You can adjust the number of epochs\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).squeeze()\n",
    "            loss = criterion(output, target.float())\n",
    "            val_loss += loss.item()\n",
    "            total += target.size(0)\n",
    "            correct += output.eq(target).sum().item()\n",
    "        \n",
    "    # Write to TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss/len(train_loader.dataset), epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss/len(val_loader.dataset), epoch)\n",
    "    writer.add_scalar('Accuracy/val', 100.*correct/total, epoch)\n",
    "\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss/len(train_loader.dataset)}, Val Loss: {val_loss/len(val_loader.dataset)}, Val Acc: {100.*correct/total}')\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ee9078c6617fa90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T11:08:18.011098572Z",
     "start_time": "2024-04-05T11:08:18.007769501Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Close TensorBoard writer\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
