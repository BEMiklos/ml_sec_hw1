{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import DatasetFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=(10,), stride=(1,))\n",
    "        self.fc1 = nn.Linear(65496, 1)  # Adjust the input size based on your data size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        x = x.view(-1, 65496)\n",
    "        return F.sigmoid(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv1d(1, 16, kernel_size=(10,), stride=(1,))\n",
       "  (fc1): Linear(in_features=65496, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNet().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryTransform:\n",
    "    def __init__(self, input_length):\n",
    "        self.input_length = input_length\n",
    "\n",
    "    def __call__(self, binary_data):\n",
    "        binary_data = np.frombuffer(binary_data, dtype=np.uint8)\n",
    "        \n",
    "        l = len(binary_data)\n",
    "\n",
    "        # Pad or truncate the binary data\n",
    "        if l < self.input_length:\n",
    "            padding = np.zeros(self.input_length - l, dtype=np.uint8)\n",
    "            binary_data = np.concatenate((binary_data, padding))\n",
    "        elif l > self.input_length:\n",
    "            excess = ceil(l / self.input_length)\n",
    "            padding = np.zeros(self.input_length * excess - l, dtype=np.uint8)\n",
    "            binary_data = np.concatenate((binary_data, padding))\n",
    "            binary_data = binary_data.reshape(len(binary_data)//excess, -1)\n",
    "            binary_data = np.mean(binary_data, axis=1)\n",
    "            \n",
    "        # Scale the data to [0, 1]\n",
    "        scaled_data = binary_data / 255.0\n",
    "        tensor = torch.tensor(scaled_data, dtype=torch.float32)\n",
    "        return tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"data/test\"\n",
    "transform = BinaryTransform(16384)\n",
    "test_dataset = DatasetFolder(\n",
    "    root=test_data_path,\n",
    "    loader=lambda x: open(x, 'rb').read(),\n",
    "    extensions=('',),\n",
    "    transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9923076923076923"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = len(test_dataset)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X).squeeze()\n",
    "        pred_label = y_pred > 0.5\n",
    "        correct += pred_label.eq(y).sum().item()\n",
    "\n",
    "correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline adversarial accuracy with random suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryTransformWithMask:\n",
    "    def __init__(self, input_length: int, adversarial_ratio: float) -> None:\n",
    "        self.input_length = input_length\n",
    "        self.adversarial_ratio = adversarial_ratio\n",
    "\n",
    "    def __call__(self, binary_data: bytes) -> Tuple[torch.Tensor, np.array]:\n",
    "        \"\"\"Returns the model input prepared as a (1,input_length) Tensor,\n",
    "        and the mask which indicates positions influenced exclusively by the adv. suffix.\"\"\"\n",
    "        l_original = len(binary_data)\n",
    "        binary_array = self.get_extended_binary_array(binary_data)\n",
    "        l_with_adversarial = len(binary_array)\n",
    "\n",
    "\n",
    "        if l_with_adversarial < self.input_length:\n",
    "            # the bytes array is too short and zero padding should be added to match input_length\n",
    "            # the mask does not include the zero-padding bytes\n",
    "            padding = np.zeros(self.input_length - l_with_adversarial, dtype=np.uint8)\n",
    "            binary_array = np.concatenate((binary_array, padding))\n",
    "            mask = np.arange(l_original, l_with_adversarial)\n",
    "        elif l_with_adversarial > self.input_length:\n",
    "            # the byte array should be split into ceil(l_with_adversarial / input_length) chunks,\n",
    "            # with the last chunk being padded to chunk size if needed\n",
    "            # when padding is used, that last chunk is not considered part of the adversarial mask \n",
    "            window_size = ceil(l_with_adversarial / self.input_length)\n",
    "            num_original_groups = ceil(l_original / window_size) # byte groups influenced by the original binary\n",
    "            l_padding = self.input_length * window_size - l_with_adversarial\n",
    "            num_padding_groups = ceil(l_padding / window_size)\n",
    "            padding = np.zeros(l_padding, dtype=np.uint8)\n",
    "            binary_array = np.concatenate((binary_array, padding))\n",
    "            binary_array = binary_array.reshape(-1, window_size)\n",
    "            binary_array = np.mean(binary_array, axis=1)\n",
    "            mask = np.arange(num_original_groups, self.input_length - num_padding_groups)\n",
    "            \n",
    "        # Scale the data to [0, 1]\n",
    "        scaled_data = binary_array / 255.0\n",
    "        tensor = torch.tensor(scaled_data, dtype=torch.float32)\n",
    "        return tensor.unsqueeze(0), mask\n",
    "\n",
    "    def get_extended_binary_array(self, binary_data: bytes) -> np.array:\n",
    "        \"\"\"Build the extended binary with the adversarial suffix set to zero.\"\"\"\n",
    "        l = len(binary_data)\n",
    "        l_with_adversarial = ceil(l * (1 + self.adversarial_ratio))\n",
    "        binary_array = np.zeros(l_with_adversarial, dtype=np.uint8)\n",
    "        binary_array[:l] = np.frombuffer(binary_data, dtype=np.uint8)\n",
    "        return binary_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0078, 0.0196, 0.0314, 0.0131, 0.0000, 0.0000]]),\n",
       " array([], dtype=int64))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example #1 in the assignment text\n",
    "binary_data = bytes(list(range(1, 11)))\n",
    "transform = BinaryTransformWithMask(input_length=6, adversarial_ratio=0.4) # +4 bytes\n",
    "X, M = transform(binary_data)\n",
    "X, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0078, 0.0196, 0.0314, 0.0131, 0.0000, 0.0000]]), array([4]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example #2 in the assignment text\n",
    "binary_data = bytes(list(range(1, 11)))\n",
    "transform = BinaryTransformWithMask(input_length=6, adversarial_ratio=0.5) # +5 bytes\n",
    "X, M = transform(binary_data)\n",
    "X, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/victim/malware/0d41d1d904aecf716303f55108e020fbd9a4dbcd997efb08fba5e10e936d419c\"\n",
    "with open(file_path, \"rb\") as f:\n",
    "    binary_data = f.read()\n",
    "\n",
    "transform = BinaryTransformWithMask(input_length=2**14, adversarial_ratio=0.05)\n",
    "input_tensor, M = transform(binary_data)\n",
    "input_tensor = input_tensor.unsqueeze(0) # add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9895]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.010592754930257797\n",
      "5 -0.012299768626689911\n",
      "10 -0.015437961556017399\n",
      "15 -0.01954956166446209\n",
      "20 -0.02464163675904274\n",
      "25 -0.03102256916463375\n",
      "30 -0.03826124221086502\n",
      "35 -0.04707461595535278\n",
      "40 -0.0577281154692173\n",
      "45 -0.07001614570617676\n",
      "50 -0.08530186116695404\n",
      "55 -0.10434142500162125\n",
      "60 -0.12705855071544647\n",
      "65 -0.15378989279270172\n",
      "70 -0.18580514192581177\n",
      "75 -0.22289367020130157\n",
      "80 -0.26724955439567566\n",
      "85 -0.3175199031829834\n",
      "90 -0.37661269307136536\n",
      "95 -0.4436857998371124\n",
      "100 -0.5188723206520081\n",
      "105 -0.6033344864845276\n",
      "110 -0.6236594319343567\n",
      "115 -0.6314519643783569\n",
      "120 -0.6379726529121399\n",
      "125 -0.6432398557662964\n",
      "130 -0.6468443870544434\n",
      "135 -0.6507037878036499\n",
      "140 -0.6554455161094666\n",
      "145 -0.6585882306098938\n",
      "150 -0.6615903377532959\n",
      "155 -0.66433185338974\n",
      "160 -0.6673728823661804\n",
      "165 -0.6690101027488708\n",
      "170 -0.6722534894943237\n",
      "175 -0.6722065210342407\n",
      "180 -0.6751646399497986\n",
      "185 -0.6743672490119934\n",
      "190 -0.6770105361938477\n",
      "195 -0.6776541471481323\n",
      "200 -0.6791898012161255\n",
      "205 -0.6796035766601562\n",
      "210 -0.6816917061805725\n",
      "215 -0.6813059449195862\n",
      "220 -0.6836955547332764\n",
      "225 -0.6825204491615295\n",
      "230 -0.6838483810424805\n",
      "235 -0.6830773949623108\n",
      "240 -0.6838677525520325\n",
      "245 -0.6826879382133484\n",
      "250 -0.6838244795799255\n",
      "255 -0.6830136775970459\n",
      "260 -0.6840959191322327\n",
      "265 -0.6831822395324707\n",
      "270 -0.683712899684906\n",
      "275 -0.682892382144928\n",
      "280 -0.6840420961380005\n",
      "285 -0.6822107434272766\n",
      "290 -0.6838639378547668\n",
      "295 -0.6826118230819702\n",
      "300 -0.6841269135475159\n",
      "305 -0.6827654242515564\n",
      "310 -0.6846179962158203\n",
      "315 -0.682365357875824\n",
      "320 -0.6847299337387085\n",
      "325 -0.6835215091705322\n",
      "330 -0.6848109364509583\n",
      "335 -0.68498295545578\n",
      "340 -0.6858735680580139\n",
      "345 -0.6849790215492249\n",
      "350 -0.6864297986030579\n",
      "355 -0.6846631169319153\n",
      "360 -0.6864036321640015\n",
      "365 -0.6848876476287842\n",
      "370 -0.6859369874000549\n",
      "375 -0.685276448726654\n",
      "380 -0.6863746643066406\n",
      "385 -0.6852074861526489\n",
      "390 -0.6864978075027466\n",
      "395 -0.686129093170166\n",
      "400 -0.6874762773513794\n",
      "405 -0.6863036155700684\n",
      "410 -0.6873449683189392\n",
      "415 -0.6859297752380371\n",
      "420 -0.6872462630271912\n",
      "425 -0.686466634273529\n",
      "430 -0.6870527267456055\n",
      "435 -0.6867307424545288\n",
      "440 -0.6869939565658569\n",
      "445 -0.6863338947296143\n",
      "450 -0.6867952942848206\n",
      "455 -0.686343252658844\n",
      "460 -0.687589168548584\n",
      "465 -0.6865819692611694\n",
      "470 -0.6882908344268799\n",
      "475 -0.6874336004257202\n",
      "480 -0.6881150603294373\n",
      "485 -0.6881201267242432\n",
      "490 -0.6881909370422363\n",
      "495 -0.6892921328544617\n",
      "500 -0.6889955401420593\n",
      "505 -0.6899545192718506\n",
      "510 -0.6901174187660217\n",
      "515 -0.6906298995018005\n",
      "520 -0.6912519931793213\n",
      "525 -0.6906391978263855\n",
      "530 -0.6913343667984009\n",
      "535 -0.6917992234230042\n",
      "540 -0.692789614200592\n",
      "545 -0.6932635307312012\n",
      "550 -0.693088173866272\n",
      "555 -0.6932826042175293\n",
      "560 -0.6935387849807739\n",
      "565 -0.6936892867088318\n",
      "570 -0.6943228840827942\n",
      "575 -0.6938557624816895\n",
      "580 -0.6944413185119629\n",
      "585 -0.6937368512153625\n",
      "590 -0.6936637163162231\n",
      "595 -0.6945923566818237\n",
      "600 -0.6941906809806824\n",
      "605 -0.6943187713623047\n",
      "610 -0.6946133375167847\n",
      "615 -0.693649172782898\n",
      "620 -0.6944792866706848\n",
      "625 -0.6944366693496704\n",
      "630 -0.6945009231567383\n",
      "635 -0.6943086385726929\n",
      "640 -0.6947069764137268\n",
      "645 -0.6942120790481567\n",
      "650 -0.6957923173904419\n",
      "655 -0.6958495378494263\n",
      "660 -0.6957250237464905\n",
      "665 -0.6955040693283081\n",
      "670 -0.6959299445152283\n",
      "675 -0.6950454711914062\n",
      "680 -0.6963237524032593\n",
      "685 -0.6952226161956787\n",
      "690 -0.6954600214958191\n",
      "695 -0.6946040391921997\n",
      "700 -0.6955955624580383\n",
      "705 -0.6947516798973083\n",
      "710 -0.6958302855491638\n",
      "715 -0.6944006085395813\n",
      "720 -0.6952719688415527\n",
      "725 -0.6948368549346924\n",
      "730 -0.6947673559188843\n",
      "735 -0.6945628523826599\n",
      "740 -0.6951958537101746\n",
      "745 -0.6939570903778076\n",
      "750 -0.6951697468757629\n",
      "755 -0.6945600509643555\n",
      "760 -0.6951376795768738\n",
      "765 -0.6946908235549927\n",
      "770 -0.6954214572906494\n",
      "775 -0.6952371597290039\n",
      "780 -0.6948860883712769\n",
      "785 -0.6951538324356079\n",
      "790 -0.6944014430046082\n",
      "795 -0.6951966881752014\n",
      "800 -0.6950386762619019\n",
      "805 -0.6954132914543152\n",
      "810 -0.6954829692840576\n",
      "815 -0.6951576471328735\n",
      "820 -0.6949818134307861\n",
      "825 -0.6948994398117065\n",
      "830 -0.694901704788208\n",
      "835 -0.6950080990791321\n",
      "840 -0.6952954530715942\n",
      "845 -0.6953244209289551\n",
      "850 -0.6955787539482117\n",
      "855 -0.6952645182609558\n",
      "860 -0.6952483057975769\n",
      "865 -0.6948965191841125\n",
      "870 -0.6950974464416504\n",
      "875 -0.694175660610199\n",
      "880 -0.6950584650039673\n",
      "885 -0.6945475935935974\n",
      "890 -0.6945727467536926\n",
      "895 -0.6951523423194885\n",
      "900 -0.6951285004615784\n",
      "905 -0.6946666240692139\n",
      "910 -0.6954323053359985\n",
      "915 -0.6948467493057251\n",
      "920 -0.694833517074585\n",
      "925 -0.6951751112937927\n",
      "930 -0.6945192813873291\n",
      "935 -0.6952630281448364\n",
      "940 -0.6950148344039917\n",
      "945 -0.6954814791679382\n",
      "950 -0.6948156356811523\n",
      "955 -0.6958845257759094\n",
      "960 -0.695270299911499\n",
      "965 -0.6950817108154297\n",
      "970 -0.695493221282959\n",
      "975 -0.6947553753852844\n",
      "980 -0.6951587200164795\n",
      "985 -0.695742666721344\n",
      "990 -0.6951435804367065\n",
      "995 -0.6956738233566284\n"
     ]
    }
   ],
   "source": [
    "adversary_features = torch.zeros(len(M), dtype=torch.float32, requires_grad=True)\n",
    "opt = torch.optim.SGD([adversary_features], lr=0.01, weight_decay=0.1)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "for t in range(1000):\n",
    "    input_with_adversary = input_tensor.clone()\n",
    "    input_with_adversary[...,M] += adversary_features\n",
    "    pred = model(input_with_adversary).squeeze()\n",
    "    loss = -loss_fn(pred, torch.tensor(1, dtype=torch.float32)) # 1 = malware\n",
    "    if t % 5 == 0:\n",
    "        print(t, loss.item())\n",
    "       \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    adversary_features.grad.sign_()\n",
    "    opt.step()\n",
    "\n",
    "    # projection with clipping\n",
    "    adversary_features.data.clamp_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4985]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_with_adversary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "        1.0000, 0.0000, 0.4762, 0.6769, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000,\n",
       "        0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
       "        1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
       "        1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.8556,\n",
       "        1.0000, 0.2929, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.8423, 0.0000,\n",
       "        0.0000, 0.0539, 0.5758, 0.2166, 0.1887, 0.0000, 0.0000, 0.0000, 0.0568,\n",
       "        0.1886, 0.0000, 0.0654, 1.0000, 0.0000, 0.0000, 0.3308, 0.0960, 0.0000,\n",
       "        0.2714, 0.0000, 0.0000, 0.0000, 0.4218, 0.0068, 0.0100, 0.0000, 0.0850,\n",
       "        0.0000, 0.1037, 0.0100, 0.0000, 0.0157, 0.0808, 0.0820, 0.0736, 0.0000,\n",
       "        0.0000, 0.0000, 0.4227, 0.1191, 0.0224, 0.0405, 0.1521, 0.9105, 0.0000,\n",
       "        0.4200, 0.2129, 0.0000, 0.1269, 0.2887, 0.0000, 0.0382, 0.1441, 0.0339,\n",
       "        0.0000, 0.1103, 0.0000, 0.0000, 0.0758, 0.0519, 0.3339, 0.1744, 0.0194,\n",
       "        0.0000, 0.1277, 0.0000, 0.4648, 0.0000, 0.1031, 0.0098, 0.0885, 0.0000,\n",
       "        0.2092, 0.0098, 0.0000, 0.0000, 0.0658, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "        0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4428,\n",
       "        0.0000, 0.0000, 0.7209, 0.0100, 0.0818, 0.0391, 0.2539, 0.0000, 0.0000,\n",
       "        0.0937, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
       "        1.0000, 0.0000, 1.0000, 0.7079, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
       "        1.0000, 1.0000, 0.6230, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
       "        1.0000, 0.6311, 0.7798, 1.0000, 0.0000, 1.0000, 0.0348, 1.0000, 0.1923,\n",
       "        0.0000, 1.0000, 1.0000, 0.0000, 0.0100, 1.0000, 0.7271, 1.0000, 0.6162,\n",
       "        0.3858, 0.1414, 1.0000, 0.5586, 0.0000, 1.0000, 0.9786, 0.4544, 1.0000,\n",
       "        1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
       "        1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9591, 0.0000, 0.2094,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
       "        0.0000, 0.0533, 0.0000, 0.0000, 0.1630, 0.0000, 0.4252, 0.4863, 1.0000,\n",
       "        0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0726, 0.0000, 0.5340, 0.0000,\n",
       "        0.0000, 0.0000, 0.0100, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "        1.0000, 1.0000, 0.0000, 0.9131, 0.9256, 0.0000, 1.0000, 0.0000, 1.0000,\n",
       "        0.0000, 0.0000, 0.4757, 0.0000, 0.0000, 0.0317, 0.1853, 0.0000, 0.0238,\n",
       "        0.0000, 0.0948, 0.0000, 0.0100, 0.0000, 0.0000, 0.0000, 0.0739, 0.0000,\n",
       "        0.0000, 0.0409, 0.0100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0575, 1.0000, 0.0100, 0.0000, 0.4395, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        1.0000, 0.0000, 1.0000, 0.6166, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "        1.0000, 0.0039, 0.0000, 0.0543, 0.0000, 0.0629, 0.0000, 0.6060, 0.0039,\n",
       "        0.0000, 0.0000, 0.5100, 0.0167, 0.2587, 0.2499, 0.0000, 0.0000, 0.3604,\n",
       "        0.1296, 0.0200, 0.0000, 0.0000, 0.1662, 1.0000, 0.0286, 0.0000, 0.3836,\n",
       "        0.9569, 0.0100, 0.3035, 0.4234, 1.0000, 0.0000, 0.0100, 0.3715, 0.0000,\n",
       "        1.0000, 0.0881, 0.0000, 1.0000, 0.0000, 0.5866, 0.0155, 0.5052, 0.0873,\n",
       "        0.0100, 0.0578, 0.0000, 0.0330, 0.1749, 0.0179, 1.0000, 0.0000, 1.0000,\n",
       "        0.0000, 0.2551, 0.0000, 0.0000, 0.0000, 0.3495, 0.1761, 0.0000, 0.0000,\n",
       "        0.3888, 0.0000, 0.0000, 0.0746, 0.0000, 0.0000, 0.0000, 0.0000, 0.0194,\n",
       "        0.0000, 0.0000, 0.4631, 0.0000, 0.0000, 0.0000, 0.1032, 0.0100, 0.0000,\n",
       "        0.0100, 0.0279, 0.0000, 0.0000, 0.0554, 0.0000, 0.0000, 0.0000, 0.0457,\n",
       "        0.0000, 0.0000, 0.0000, 0.1904, 0.0000, 0.0000, 0.0100, 0.0000, 0.0100,\n",
       "        0.1238, 0.0794, 0.0000, 0.0000, 0.0533, 0.0483, 0.0832, 0.0000, 0.0000,\n",
       "        0.0296, 0.0757, 0.1684, 0.1741, 0.0000, 0.2463, 0.0000, 0.0100, 0.1675,\n",
       "        0.0000, 0.0000, 1.0000, 0.0342, 0.0000, 0.5050, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.5937, 0.0000, 0.0000, 0.0100, 0.0000,\n",
       "        0.4329, 0.0000, 0.0000, 0.3014, 0.5561, 0.0000, 0.3443, 1.0000, 0.0000,\n",
       "        1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000,\n",
       "        0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
       "        1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "        0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "        1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
       "        1.0000, 0.0924, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
       "        0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
       "        0.0000, 1.0000, 0.0000, 1.0000, 0.3142, 1.0000, 0.6013, 0.0000, 0.0000,\n",
       "        0.5952, 0.0000, 0.0000, 0.0000, 0.1392, 0.0000, 0.1978, 0.0165, 0.0850,\n",
       "        0.0000, 0.0000, 0.0000, 0.0920, 0.0100, 0.0100, 0.4322, 1.0000, 0.0000,\n",
       "        1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.4104, 0.5291, 0.1186, 0.0000,\n",
       "        1.0000, 0.2606, 0.3727, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.6180,\n",
       "        1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "        0.0000, 0.4901, 0.0000, 0.3048, 0.0000, 0.0000, 0.0000, 0.0867, 0.0000,\n",
       "        0.0100, 0.0000, 0.5221, 0.0000, 0.0000, 0.0000, 0.2748, 0.0000, 0.1442,\n",
       "        0.0000, 0.2396, 0.0000, 0.0462, 0.0000, 0.1347, 0.0000, 0.0000, 0.0000,\n",
       "        0.0412, 0.0100, 0.0000, 0.0100, 0.3005, 0.0000, 0.0100, 0.0218, 0.0389,\n",
       "        0.0000, 0.1570, 0.0100, 0.0000, 0.0100, 0.0000, 0.0100, 0.1348, 0.1165,\n",
       "        0.0198, 0.0000, 0.2006, 0.0117, 0.0000, 0.0000, 0.0000, 0.1491, 0.5322,\n",
       "        0.0000, 0.2882, 0.0000, 0.3234, 0.1661, 0.1874, 0.0000, 0.2875, 0.0000,\n",
       "        0.0100, 0.0451, 0.0256, 0.0000, 0.4613, 0.0148, 0.0000, 0.1535, 0.0000,\n",
       "        0.0000, 1.0000, 0.0100, 0.5173, 0.0000, 1.0000, 0.0884, 0.0000, 0.0000,\n",
       "        0.0100, 1.0000, 0.0100, 0.0000, 0.0000, 1.0000, 0.0000, 0.0399, 0.0000,\n",
       "        0.0000, 0.1010, 0.0100, 0.0000, 0.0940, 0.0081, 0.0000, 0.0895, 0.0686,\n",
       "        0.0100, 0.0065, 0.0746, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0610,\n",
       "        0.1495, 1.0000, 0.0000, 1.0000, 1.0000], requires_grad=True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
